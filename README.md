# COMPILER-DESIGN-BASICS
*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: SRIBALAJI

*INTERN ID*: CT04DY1002

*DOMAIN*: C PROGRAMMING

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTHOSH

The program provided is a simple implementation of a lexical analyzer in the C programming language, and it represents one of the fundamental building blocks of compiler design. The purpose of this program is to take a source code file as input, scan it character by character, and classify different parts of the code into categories such as keywords, identifiers, and operators. This process is called lexical analysis, and it is the very first stage of compilation. Without lexical analysis, a compiler would not be able to break down the raw text of a program into meaningful components, making it extremely difficult to process further. In this program, the source code is read from a file named input.txt, and the program outputs each recognized token along with its classification.
The implementation begins with the definition of an array containing a list of C language keywords. These are words such as “int,” “float,” “if,” “else,” “while,” and “return,” which are reserved by the language and have specific meanings. The function isKeyword() is used to determine whether a given word matches any of these reserved keywords. Another helper function, isOperator(), is used to check whether a single character belongs to the category of operators, which include arithmetic operators like “+” and “-”, relational operators like “<” and “>”, and logical symbols like “&” and “|.” By using these functions, the program can distinguish between different types of tokens in the input file.
The program was developed and executed using Visual Studio Code (VS Code), a popular code editor created by Microsoft. VS Code is a powerful and lightweight source code editor that supports a wide variety of programming languages, including C and C++. It is available on Windows, Linux, and macOS, making it highly accessible to programmers across platforms. One of the key features of VS Code is its extension system. For C programming, the C/C++ extension by Microsoft enables features like syntax highlighting, code navigation, IntelliSense (auto-completion), and debugging tools. With the help of the integrated terminal, users can easily compile C code using GCC (GNU Compiler Collection) or any other compiler configured in their system. VS Code also supports version control through Git, allowing developers to track changes in their code efficiently. VS Code is particularly suitable for beginners and intermediate programmers due to its user-friendly interface and robust feature set. It is widely used in academic institutions for teaching programming and software development.
The main logic of the program involves opening the input file and then scanning it character by character. Whitespace characters such as spaces, tabs, and newlines are ignored, since they do not contribute to the tokens. If the program encounters a character that qualifies as an operator, it is immediately classified and printed. If the program encounters an alphabetic character, it begins reading it as part of a word. It continues reading until it reaches a non-alphanumeric character. Once a word is complete, the program checks if the word is one of the predefined keywords. If so, it is labeled as a keyword; if not, it is classified as an identifier. Identifiers are the names that programmers create for variables, functions, or other entities in their code. This process continues until the entire file has been read and analyzed.
The usefulness of this task extends to many areas, with the most prominent being compiler design. Every modern compiler uses a lexical analyzer to tokenize the source code. The tokens produced by the lexical analyzer are then passed to the parser, which constructs the grammatical structure of the code based on predefined rules. By dividing the source code into tokens, lexical analysis simplifies the work of the parser and makes it easier to detect syntax errors. This division into tokens also allows compilers to generate intermediate and machine-level code in later phases. In this way, lexical analyzers serve as the foundation of the entire compilation process.
Beyond compiler design, lexical analysis is also useful in interpreters and scripting languages. Interpreters for languages like Python and JavaScript must tokenize code before executing it line by line. Text processing tools and code editors also rely heavily on lexical analysis. Features such as syntax highlighting in code editors are made possible because the editor analyzes the code to identify keywords, identifiers, and operators, and then applies different colors or styles to them. Similarly, linters and formatters, which check code for errors or enforce consistent style, use lexical analysis to interpret the structure of the code.
Outside programming languages, similar techniques are used in natural language processing and search engines. In these cases, the process is often called tokenization, but the underlying idea is the same: breaking a stream of text into meaningful units that can be processed further. For example, a search engine tokenizes user queries and documents to match keywords efficiently. In natural language processing, tokenization is the first step before applying more advanced techniques like part-of-speech tagging or machine translation.
In an educational context, this program is particularly valuable. It is commonly included in compiler design laboratory courses because it gives students practical experience with the first stage of compilation. Through writing such a program, students gain insights into how high-level programming languages are interpreted by machines, and they also practice important skills such as working with files, strings, and classification logic in C. Even though the program is simplified and does not yet handle all possible tokens such as numbers, separators, or multi-character operators, it conveys the essential principles clearly.
In conclusion, this program is a simple but effective demonstration of lexical analysis. By reading source code, classifying parts of it as keywords, identifiers, or operators, and displaying the results, it illustrates how compilers begin to process high-level language input. The usefulness of this task goes far beyond a classroom exercise. It forms the basis for compiler design, supports interpreters, powers text processing tools, and connects to many other areas of computing where tokenization is required. For students and developers alike, understanding and implementing such a program provides a strong foundation in both programming and language processing.

output:

<img width="1635" height="772" alt="Image" src="https://github.com/user-attachments/assets/e6ece104-c957-4be6-88ae-521f0e773c57" />
